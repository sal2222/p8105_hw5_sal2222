---
title: "p8105_hw5_sal2222"
author: "Stephen Lewandowski"
date: "November 9, 2018"
output: 
  github_document:
    toc: true
---

```{r setup, include = FALSE}

library(tidyverse)
library(readxl)


knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```

## Problem 1 - Arm longitudinal study 

For this problem, I will create a tidy dataframe from all participants, including the subject ID, arm, and observations over time. The raw data includes 20 files, one for each subject, each with eight weeks of observations arranged in wide format.

I made a function to read-in the spreadsheet files and transform the data in each file from wide to long by week.

I then applied the function to each file name using `map_dfr` from the `purr` package and extracted the arm and subject ID variables from the file name.

```{r import_and_tidy_arm}

# create vector of file names to load
files <- list.files(path = "./data", pattern = "*.csv", full.names = TRUE) %>% 
  set_names()

# create function to import and transform a file
import_file <- function(filename) 
{ 
  arm_file <- read.csv(file = filename) %>% 
     gather(key = week, value = value)
} 

# map file vector over the import function and tidy tibble
arm_df <- as_tibble(
  map_dfr(.x = files, import_file, .id = "file_name") %>% 
  separate(file_name, into = c("remove_1", "remove_2", "temp_1"), sep = "/") %>%
  mutate(arm_id = str_replace(temp_1, ".csv", ""),
         week = factor(str_replace(week, "week_", ""))) %>% 
  separate(arm_id, into = c("arm", "subject_id"), sep = "_", remove = FALSE) %>% 
  select(c("arm_id", "arm", "subject_id", "week",  "value")) %>% 
  mutate(subject_id = as.integer(subject_id),
         arm_id = factor(arm_id),
         arm = factor(arm))  
)       
arm_df

  
```


I will make a spaghetti plot showing observations on each subject over time, and to observe differences between groups.


```{r arm_spaghetti_plot}

arm_df %>%
  ggplot(aes(x = week, y = value, group = arm_id, color = arm)) +
    geom_line() + 
    labs(
      title = "Observations over time",
      x = "week",
      y = "observation value",
      caption = "Data from a longitudinal study with control arm and experimental arm"
    ) + 
    viridis::scale_color_viridis(
      name = "Arm", 
      discrete = TRUE
    )

```

At the group level, experimental arm observation values were higher than control arm values. The experimental arm values increased over time, while the control arm values did not.


## Problem 2 - Homicides in U.S. cities


This problem includes data from the Washington Post on homicides in 50 large U.S. cities. 

 


Describe the raw data. Create a city_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

```{r}
homicides <- read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

homicides <- as.tibble(homicides) %>% 
  mutate(city_state = str_c(city, state, sep = ", ", collapse = NULL))

homicides

```


```{r inspect, results = FALSE}
skimr::skim(homicides)
```

In the raw data, the 50 included cities represent 28 unique states. The dataset contains information on a total of `r nrow(homicides)` homicide cases. The cities with the most homicides are Chicago (5,535), Philadelphia (3,057), Houston (2,942), and Baltimore (2,827). The states with the most cases are California, Texas, Illinois, and Pennsylvania. 

It contains victim information including name, race, age, and gender, along with the reported date of the homicide, grid coordinates for the location (missing for 60 cases), and case disposition status. The date range spans from January 2007 to November 2015. 


For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.


```{r Baltimore_prop_test}

homicides %>% 
  filter(city == "Baltimore") %>% 
  select(disposition) %>% 
  count(disposition)



levels(homicides$disposition)
```

prop.test(x, n, p = NULL,
          alternative = c("two.sided", "less", "greater"),
          conf.level = 0.95, correct = TRUE)


Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and  unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.





Create a plot that shows the estimates and CIs for each city – check out geom_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.

