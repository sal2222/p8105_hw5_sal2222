p8105\_hw5\_sal2222
================
Stephen Lewandowski
November 9, 2018

-   [Problem 1 - Arm longitudinal study](#problem-1---arm-longitudinal-study)
-   [Problem 2 - Homicides in U.S. cities](#problem-2---homicides-in-u.s.-cities)

Problem 1 - Arm longitudinal study
----------------------------------

For this problem, I will create a tidy dataframe from all participants, including the subject ID, arm, and observations over time. The raw data includes 20 files, one for each subject, each with eight weeks of observations arranged in wide format.

I made a function to read-in the spreadsheet files and transform the data in each file from wide to long by week.

I then applied the function to each file name using `map_dfr` from the `purr` package and extracted the arm and subject ID variables from the file name.

``` r
# create vector of file names to load
files <- list.files(path = "./data", pattern = "*.csv", full.names = TRUE) %>% 
  set_names()

# create function to import and transform a file
import_file <- function(filename) 
{ 
  arm_file <- read.csv(file = filename) %>% 
     gather(key = week, value = value)
} 

# map file vector over the import function and tidy tibble
arm_df <- as_tibble(
  map_dfr(.x = files, import_file, .id = "file_name") %>% 
  separate(file_name, into = c("remove_1", "remove_2", "temp_1"), sep = "/") %>%
  mutate(arm_id = str_replace(temp_1, ".csv", ""),
         week = factor(str_replace(week, "week_", ""))) %>% 
  separate(arm_id, into = c("arm", "subject_id"), sep = "_", remove = FALSE) %>% 
  select(c("arm_id", "arm", "subject_id", "week",  "value")) %>% 
  mutate(subject_id = as.integer(subject_id),
         arm_id = factor(arm_id),
         arm = factor(arm))  
)       
arm_df
```

    ## # A tibble: 160 x 5
    ##    arm_id arm   subject_id week  value
    ##    <fct>  <fct>      <int> <fct> <dbl>
    ##  1 con_01 con            1 1      0.2 
    ##  2 con_01 con            1 2     -1.31
    ##  3 con_01 con            1 3      0.66
    ##  4 con_01 con            1 4      1.96
    ##  5 con_01 con            1 5      0.23
    ##  6 con_01 con            1 6      1.09
    ##  7 con_01 con            1 7      0.05
    ##  8 con_01 con            1 8      1.94
    ##  9 con_02 con            2 1      1.13
    ## 10 con_02 con            2 2     -0.88
    ## # ... with 150 more rows

I will make a spaghetti plot showing observations on each subject over time, and to observe differences between groups.

``` r
arm_df %>%
  ggplot(aes(x = week, y = value, group = arm_id, color = arm)) +
    geom_line() + 
    labs(
      title = "Observations over time",
      x = "week",
      y = "observation value",
      caption = "Data from a longitudinal study with control arm and experimental arm"
    ) + 
    viridis::scale_color_viridis(
      name = "Arm", 
      discrete = TRUE
    )
```

<img src="p8105_hw5_sal2222_files/figure-markdown_github/arm_spaghetti_plot-1.png" width="90%" />

At the group level, experimental arm observation values were higher than control arm values. The experimental arm values increased over time, while the control arm values did not.

Problem 2 - Homicides in U.S. cities
------------------------------------

This problem includes data from the Washington Post on homicides in 50 large U.S. cities.

Describe the raw data. Create a city\_state variable (e.g. “Baltimore, MD”) and then summarize within cities to obtain the total number of homicides and the number of unsolved homicides (those for which the disposition is “Closed without arrest” or “Open/No arrest”).

``` r
homicides <- read.csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")

homicides <- as.tibble(homicides) %>% 
  mutate(city_state = str_c(city, state, sep = ", ", collapse = NULL))

homicides
```

    ## # A tibble: 52,179 x 13
    ##    uid   reported_date victim_last victim_first victim_race victim_age
    ##    <fct>         <int> <fct>       <fct>        <fct>       <fct>     
    ##  1 Alb-~      20100504 GARCIA      JUAN         Hispanic    78        
    ##  2 Alb-~      20100216 MONTOYA     CAMERON      Hispanic    17        
    ##  3 Alb-~      20100601 SATTERFIELD VIVIANA      White       15        
    ##  4 Alb-~      20100101 MENDIOLA    CARLOS       Hispanic    32        
    ##  5 Alb-~      20100102 MULA        VIVIAN       White       72        
    ##  6 Alb-~      20100126 BOOK        GERALDINE    White       91        
    ##  7 Alb-~      20100127 MALDONADO   DAVID        Hispanic    52        
    ##  8 Alb-~      20100127 MALDONADO   CONNIE       Hispanic    52        
    ##  9 Alb-~      20100130 MARTIN-LEY~ GUSTAVO      White       56        
    ## 10 Alb-~      20100210 HERRERA     ISRAEL       Hispanic    43        
    ## # ... with 52,169 more rows, and 7 more variables: victim_sex <fct>,
    ## #   city <fct>, state <fct>, lat <dbl>, lon <dbl>, disposition <fct>,
    ## #   city_state <chr>

``` r
skimr::skim(homicides)
```

In the raw data, the 50 included cities represent 28 unique states. The dataset contains information on a total of 52179 homicide cases. The cities with the most homicides are Chicago (5,535), Philadelphia (3,057), Houston (2,942), and Baltimore (2,827). The states with the most cases are California, Texas, Illinois, and Pennsylvania.

It contains victim information including name, race, age, and gender, along with the reported date of the homicide, grid coordinates for the location (missing for 60 cases), and case disposition status. The date range spans from January 2007 to November 2015.

For the city of Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved; save the output of prop.test as an R object, apply the broom::tidy to this object and pull the estimated proportion and confidence intervals from the resulting tidy dataframe.

``` r
homicides %>% 
  filter(city == "Baltimore") %>% 
  select(disposition) %>% 
  count(disposition)
```

    ## # A tibble: 3 x 2
    ##   disposition               n
    ##   <fct>                 <int>
    ## 1 Closed by arrest       1002
    ## 2 Closed without arrest   152
    ## 3 Open/No arrest         1673

``` r
levels(homicides$disposition)
```

    ## [1] "Closed by arrest"      "Closed without arrest" "Open/No arrest"

prop.test(x, n, p = NULL, alternative = c("two.sided", "less", "greater"), conf.level = 0.95, correct = TRUE)

Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each. Do this within a “tidy” pipeline, making use of purrr::map, purrr::map2, list columns and unnest as necessary to create a tidy dataframe with estimated proportions and CIs for each city.

Create a plot that shows the estimates and CIs for each city – check out geom\_errorbar for a way to add error bars based on the upper and lower limits. Organize cities according to the proportion of unsolved homicides.
